import os
import streamlit as st
import chromadb
from openai import OpenAI
import requests

# --- å‡†å¤‡å·¥ä½œ (ä¿æŒä¸å˜) ---
SILICONFLOW_API_KEY = os.environ.get("SILICONFLOW_API_KEY")

def load_css(file_name):
    with open(file_name, "r", encoding="utf-8") as f:
        st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)

# (åç«¯æ ¸å¿ƒå‡½æ•°éƒ¨åˆ†ä¸ä¹‹å‰å®Œå…¨ç›¸åŒï¼Œä¸ºèŠ‚çœç¯‡å¹…çœç•¥)
@st.cache_resource
def initialize_database():
    project_root = os.path.dirname(os.path.abspath(__file__))
    db_path = os.path.join(project_root, "nier_vector_db")
    client = chromadb.PersistentClient(path=db_path)
    collection = client.get_or_create_collection(name="nier_automata_kb")
    return collection

@st.cache_data
def get_embedding(text_chunk):
    url = "https://api.siliconflow.cn/v1/embeddings"
    model_name = "BAAI/bge-m3"
    headers = { "Authorization": f"Bearer {SILICONFLOW_API_KEY}", "Content-Type": "application/json" }
    payload = { "model": model_name, "input": text_chunk }
    try:
        response = requests.post(url, json=payload, headers=headers)
        if response.status_code == 200:
            return response.json()['data'][0]['embedding']
    except Exception:
        return None
    return None

@st.cache_resource
def get_llm_client():
    return OpenAI(api_key=SILICONFLOW_API_KEY, base_url="https://api.siliconflow.cn/v1")

def generate_answer_stream(query, retrieved_chunks, llm_client):
    context = "\n\n---\n\n".join(retrieved_chunks)
    prompt = f"""
è¯·ä½ æ‰®æ¼”ä¸€ä¸ªã€Šå°¼å°”ï¼šæœºæ¢°çºªå…ƒã€‹çš„èµ„æ·±ä¸“å®¶ã€‚
ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ä¸‹é¢æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç®€æ´ã€æ¸…æ™°åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
ã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‘:
{context}
ã€ç”¨æˆ·çš„é—®é¢˜ã€‘:
{query}
"""
    model_name = "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B" 
    stream = llm_client.chat.completions.create(
        model=model_name,
        messages=[{"role": "user", "content": prompt}],
        stream=True
    )
    return stream


# --- âœ¨âœ¨âœ¨ æœ€ç»ˆå®Œç¾ç‰ˆ UI âœ¨âœ¨âœ¨ ---

st.set_page_config(page_title="å°¼å°”AIä¸‡äº‹é€š", page_icon="ğŸ¤–", layout="centered")
load_css("style.css")

if not SILICONFLOW_API_KEY:
    st.error("é”™è¯¯ï¼šæœªèƒ½åœ¨ç³»ç»Ÿç¯å¢ƒå˜é‡ä¸­æ‰¾åˆ° 'SILICONFLOW_API_KEY'ã€‚")
    st.stop()

collection = initialize_database()
llm_client = get_llm_client()

st.title("å°¼å°”AIä¸‡äº‹é€š")
st.caption("ä¸€ä¸ªåŸºäºRAGæŠ€æœ¯çš„ã€Šå°¼å°”ï¼šæœºæ¢°çºªå…ƒã€‹é—®ç­”æœºå™¨äºº")

# --- åˆå§‹åŒ–ä¸å†å²è®°å½•æ˜¾ç¤º ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# âœ¨ å…¨æ–°ã€ç»Ÿä¸€çš„æ¸²æŸ“é€»è¾‘ âœ¨
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æ„åŒ–çš„AIå›ç­”
        if message["role"] == "assistant" and "final_answer" in message:
            # æ¸²æŸ“æ£€ç´¢ç‰‡æ®µ
            if message.get("retrieved"):
                with st.expander("ğŸ” æŸ¥çœ‹æ£€ç´¢åˆ°çš„çŸ¥è¯†ç‰‡æ®µ", expanded=False):
                    for i, metadata in enumerate(message["retrieved"]["metadatas"]):
                        st.write(f"**ç‰‡æ®µ {i+1} (æ¥æº: {metadata['source']})**")
                        st.caption(message["retrieved"]["documents"][i])
            
            # æ¸²æŸ“æ€è€ƒè¿‡ç¨‹
            if message.get("reasoning"):
                with st.expander("ğŸ¤” æŸ¥çœ‹AIçš„æ€è€ƒè¿‡ç¨‹", expanded=False):
                    st.markdown(message["reasoning"])

            # æ¸²æŸ“æœ€ç»ˆç­”æ¡ˆï¼Œå¹¶åœ¨å‰é¢åŠ ä¸€æ¡åˆ†å‰²çº¿
            st.markdown("---")
            st.markdown(message["final_answer"])
        else:
            # æ¸²æŸ“ç®€å•çš„ç”¨æˆ·æ¶ˆæ¯
            st.markdown(message["content"])


# --- è·å–ç”¨æˆ·è¾“å…¥å¹¶å¼€å§‹æ–°çš„äº¤äº’æµç¨‹ ---
if user_prompt := st.chat_input("å…³äºã€Šå°¼å°”ï¼šæœºæ¢°çºªå…ƒã€‹ï¼Œä½ æœ‰ä»€ä¹ˆæƒ³é—®çš„ï¼Ÿ"):
    st.session_state.messages.append({"role": "user", "content": user_prompt})
    with st.chat_message("user"):
        st.markdown(user_prompt)

    with st.chat_message("assistant"):
        # æ­¥éª¤1: RAGæ£€ç´¢
        query_vector = get_embedding(user_prompt)
        if query_vector is None:
            st.error("é—®é¢˜ç†è§£å¤±è´¥ã€‚")
            st.stop()
        
        results = collection.query(query_embeddings=[query_vector], n_results=3)
        retrieved_documents = results['documents'][0]
        retrieved_metadatas = results['metadatas'][0]

        # æ­¥éª¤2: ç«‹å³æ˜¾ç¤ºæ£€ç´¢ç»“æœ
        with st.expander("ğŸ” æŸ¥çœ‹æ£€ç´¢åˆ°çš„çŸ¥è¯†ç‰‡æ®µ", expanded=True):
            for i, metadata in enumerate(retrieved_metadatas):
                st.write(f"**ç‰‡æ®µ {i+1} (æ¥æº: {metadata['source']})**")
                st.caption(retrieved_documents[i])
        
        # æ­¥éª¤3: æµå¼ç”Ÿæˆ
        thinking_placeholder = st.empty()
        answer_placeholder = st.empty()
        
        reasoning_text = ""
        final_answer = ""
        
        try:
            stream = generate_answer_stream(user_prompt, retrieved_documents, llm_client)
            
            is_thinking_phase = True
            for chunk in stream:
                if not chunk.choices: continue
                delta = chunk.choices[0].delta

                if delta.reasoning_content:
                    reasoning_text += delta.reasoning_content
                    thinking_placeholder.markdown(f"ğŸ¤” **AIæ­£åœ¨æ€è€ƒ...**\n\n---\n{reasoning_text}â–Œ")
                
                elif delta.content:
                    if is_thinking_phase:
                        is_thinking_phase = False
                        thinking_placeholder.expander("ğŸ¤” æŸ¥çœ‹AIçš„æ€è€ƒè¿‡ç¨‹", expanded=False).markdown(reasoning_text)
                    
                    final_answer += delta.content
                    answer_placeholder.markdown(f"ğŸ¤– **å°¼å°”AIä¸‡äº‹é€šçš„å›ç­”**\n\n---\n{final_answer}â–Œ")

            if final_answer:
                answer_placeholder.markdown(f"ğŸ¤– **å°¼å°”AIä¸‡äº‹é€šçš„å›ç­”**\n\n---\n{final_answer}")

        except Exception as e:
            st.error(f"ç”Ÿæˆå›ç­”æ—¶é‡åˆ°é—®é¢˜ï¼š{e}")

    # âœ¨ å…¨æ–°ã€ç»“æ„åŒ–çš„å­˜å‚¨é€»è¾‘ âœ¨
    assistant_message = {
        "role": "assistant",
        "retrieved": {
            "documents": retrieved_documents,
            "metadatas": retrieved_metadatas
        },
        "reasoning": reasoning_text,
        "final_answer": final_answer
    }
    st.session_state.messages.append(assistant_message)